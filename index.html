<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Lock-Free Priority-Aware Work-Stealing Deque</title>
  <style>
    body {
      font-family: 'Segoe UI', sans-serif;
      line-height: 1.6;
      padding: 2rem;
      background-color: #f5f7fa;
      color: #222;
    }
    h1, h2 {
      color: #2c3e50;
    }
    h1 {
      margin-bottom: 0.2em;
    }
    h2 {
      margin-top: 1.5em;
      border-bottom: 2px solid #ddd;
      padding-bottom: 0.3em;
    }
    pre {
      background-color: #efefef;
      padding: 1em;
      overflow-x: auto;
      font-size: 0.9em;
    }
    code {
      font-family: Consolas, monospace;
    }
    ul {
      padding-left: 1.5em;
    }
    .section {
      margin-bottom: 2em;
    }
  </style>
</head>
<body>
  <h1>Lock-Free, Priority-Aware Work-Stealing Deque</h1>

  <div class="section">
    <h2>Summary</h2>
    <p>We intend to build a lock-free, priority-aware work-stealing deque for dynamic task scheduling in parallel computing systems. Each worker thread maintains a priority deque and pushes/pops tasks locally, while stealing tasks from others based on priority. This is achieved using atomic operations (<code>compare_exchange</code>, <code>fetch_add</code>) in OpenMP and optionally MPI. We aim to benchmark the implementation on shared-memory (8–128 cores) and multi-node systems using MPI, testing throughput, steal attempts, and priority correctness.</p>
  </div>

  <div class="section">
    <h2>Background</h2>
    <p>Work stealing allows idle threads to dynamically balance load by stealing tasks from others. Traditional deques are extended here with priority awareness. Atomic operations replace locks to avoid contention. This system supports fine-grained concurrency, priority-driven task selection, and hybrid OpenMP+MPI environments for full scalability. Benchmarking includes throughput, steal success rate, and accuracy of priority execution.</p>
  </div>

  <div class="section">
    <h2>Pseudocode</h2>
    <pre><code>while not done:
    task = pop_local_highest_priority_task()
    if task != NULL:
        execute(task)
    else:
        victim = random_other_thread()
        task = steal_lowest_priority_task_from(victim)
        if task != NULL:
            execute(task)
        else:
            spin_wait()</code></pre>
  </div>

  <div class="section">
    <h2>The Challenge</h2>
    <p>Each thread handles 1000 tasks with mixed priorities. After executing <code>k</code> tasks, it may steal from others. We compare two approaches:</p>
    <ul>
      <li><strong>Thread-local deques:</strong> Efficient but synchronization overhead on status propagation.</li>
    </ul>
    <p>Stealing with priorities can cause inversion if not carefully controlled. We explore ways to balance fairness, correctness, and efficiency under saturation and priority skew.</p>
  </div>

  <div class="section">
    <h2>Resources</h2>
    <p>Our implementation will be in C/C++ and optionally MPI (distributed memory). We refer to literature on lock-free data structures and atomics, and consult documentation on <code>std::atomic</code>, <code>__sync_*</code>, and <code>__atomic_*</code> intrinsics. Testing is done on multi-core machines and GHC clusters.</p>
  </div>

<div class="section">
  <h2>Plan to Achieve</h2>
  <ul>
    <li style="background-color: #d4edda;"><strong>Amogh + Akshay:</strong> Implement a deque with local push/pop and remote steal operations. Initially, the part of stealing the work, updating the queue, and processing elements is done sequentially.</li>
    <li style="background-color: #d4edda;"><strong>Akshay + Amogh:</strong> Introduce lock-free atomic building blocks (e.g., <code>compare_exchange</code>, <code>fetch_add</code>) to avoid synchronization locks across threads in stealing tasks from the deque.</li>
    <li><s><strong>Compare different priority-handling approaches:</strong> separate deques by priority level vs. single-deque combination of priority tagging.</s></li>
    <li><strong>Amogh + Akshay:</strong> Extend the deque to have <code>n</code> deques per processor, each storing tasks tagged with its priority. Operations should be lock-free, and task stealing should choose the best priority task available in that processor.</li>
    <li><strong>Amogh + Akshay:</strong> Extend the stealing logic to choose from the queue populated with the best-priority tasks.</li>
    <li><strong>Akshay:</strong> Measure the tradeoff between stealing 1 task vs. <code>n</code> tasks at once, analyzing time taken, synchronization overhead, communication time, and other relevant metrics.</li>
    <li><strong>Amogh:</strong> Perform extensive benchmarking on performance characteristics such as task throughput, steal attempts, priority inversion, idle time, and scaling behavior.</li>
    <li><strong>Amogh:</strong> Compare tradeoffs when tasks are re-stolen and analyze the resulting overhead and its effect on total execution time.</li>
    <li><strong>Akshay:</strong> Benchmark results against a baseline with random (non-priority) stealing to evaluate the benefit of priority-based approaches.</li>
    <li><em>Nice to have:</em> <strong>Amogh + Akshay:</strong> Implement batching logic where each processor, after executing <code>n</code> tasks, attempts to steal higher-priority tasks from random processors. Evaluate whether this improves high-priority task completion without harming low-priority tasks.</li>
    <li><em>Nice to have:</em> <strong>Akshay + Amogh:</strong> Implement a distributed-memory version using MPI, where each process represents a node with local worker threads and simulates stealing across nodes via message passing.</li>
  </ul>
</div>


  <div class="section">
    <h2>Platform</h2>
    <p>x86-64 multi-core architecture C/C++ for implementation.</p>
  </div>

  <div class="section">
    <h2>Rough Schedule</h2>
  <ul>
    <li><strong><s>Week 1:</s></strong> <s>Setup, approach design finalise</s></li>
    <li><strong><s>Week 2:</s></strong> <s>Lock free implementation for the dequeue complete</s></li>
    <li><strong><s>Week 3 (First Half):</s></strong> <s>Extend implementation to handle <code>n</code> deques in a lock-free way with each deque having tasks of different priority.</s></li>
    <li><strong><s>Week 3 (Second Half):</s></strong> <s>Complete the implementation and task stealing logic of stealing the highest priority task from a random processor.</s></li>
     <li><strong>Week 4 (First Half) (Nice to have):</strong> Implement logic where each processor, after performing <code>n</code> tasks, tries to steal higher-priority tasks from a random processor’s queues.</li>
    <li><strong><s>Week 4 (Full):</s></strong> <s>Benchmark all components, generate visualizations, and prepare the final report and poster-day materials. This runs in parallel with the nice-to-have goal, if possible.</s></li>

  </ul>
</div>

  <div class="section">
  <h2>Final Approach</h2>
  <p>
    Each processor in our system maintains a set of independent lock-free deques, with each deque dedicated to tasks of a fixed priority level; in our implementation, we use three priority levels. These deques are implemented using CAS operations, enabling processors to push and pop tasks without locks — local operations occur at one end of the deque, while remote processors steal from the opposite end, reducing contention. To ensure correct memory ordering and visibility of updates across all cores, we employ explicit memory barriers using <code>__sync_synchronize()</code> at key points in the push, pop, and steal operations. Each processor keeps track of the current priority level it is executing and initially works on tasks from its highest-priority deque. When a processor’s local deque for the current priority becomes empty, it attempts to steal tasks of the same priority from other processors. If no tasks are found after probing all peers, the processor increments its local priority and begins working on the next lower level. CAS failures during stealing, which may occur due to contention, are tolerated by the system; a failed CAS does not guarantee that the deque is empty, only that another processor accessed it concurrently. This design accepts occasional priority inversions in favor of minimizing overhead from excessive retries, maintaining eventual consistency as processors repeatedly probe for work. Our implementation carefully balances the overhead of aggressive stealing with the need to enforce strong priority execution guarantees, and we evaluate this balance using metrics such as CAS failure rates, priority inversion occurrences, and average stealing effort.
  </p>
</div>

  <div class="section">
  <h2>Conclusion</h2>
  <p>
    Our project implements a scalable, lock-free work-stealing task system designed to efficiently distribute work across multiple processor cores. We evaluated system performance by measuring the total execution time required to complete a fixed set of tasks with varying workloads and distrbution on systems with varying core counts, specifically 4, 8, and 12 cores for most tests. The results demonstrate strong scalability when increasing from 4 to 8 cores, with execution times decreasing by more than half. This shows excellent parallel efficiency, where the benefits of parallelizing work significantly outweigh the overhead from synchronization and task-stealing operations. However, for fewer number of tasks, the performance gains diminish. For larger tasks with higher number of cores the execution time shows good improvement with less overhead cost. As the system scales, it transitions from being computation-bound to overhead-bound if the workload size does not proportionally increase. Our findings highlight the importance of balancing task granularity, workload size, and core count when designing scalable parallel systems. We also benchmarked all our tests with a single lock free dequeue which is not priority aware and our results show the overhead of our solution is 15-25 percent across workloads.
  </p>
</div>



</body>
</html>

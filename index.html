<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Lock-Free Priority-Aware Work-Stealing Deque</title>
  <style>
    body {
      font-family: 'Segoe UI', sans-serif;
      line-height: 1.6;
      padding: 2rem;
      background-color: #f5f7fa;
      color: #222;
    }
    h1, h2 {
      color: #2c3e50;
    }
    h1 {
      margin-bottom: 0.2em;
    }
    h2 {
      margin-top: 1.5em;
      border-bottom: 2px solid #ddd;
      padding-bottom: 0.3em;
    }
    pre {
      background-color: #efefef;
      padding: 1em;
      overflow-x: auto;
      font-size: 0.9em;
    }
    code {
      font-family: Consolas, monospace;
    }
    ul {
      padding-left: 1.5em;
    }
    .section {
      margin-bottom: 2em;
    }
  </style>
</head>
<body>
  <h1>Lock-Free, Priority-Aware Work-Stealing Deque</h1>

  <div class="section">
    <h2>Summary</h2>
    <p>We intend to build a lock-free, priority-aware work-stealing deque for dynamic task scheduling in parallel computing systems. Each worker thread maintains a priority deque and pushes/pops tasks locally, while stealing tasks from others based on priority. This is achieved using atomic operations (<code>compare_exchange</code>, <code>fetch_add</code>) in OpenMP and optionally MPI. We aim to benchmark the implementation on shared-memory (8–128 cores) and multi-node systems using MPI, testing throughput, steal attempts, and priority correctness.</p>
  </div>

  <div class="section">
    <h2>Background</h2>
    <p>Work stealing allows idle threads to dynamically balance load by stealing tasks from others. Traditional deques are extended here with priority awareness. Atomic operations replace locks to avoid contention. This system supports fine-grained concurrency, priority-driven task selection, and hybrid OpenMP+MPI environments for full scalability. Benchmarking includes throughput, steal success rate, and accuracy of priority execution.</p>
  </div>

  <div class="section">
    <h2>Pseudocode</h2>
    <pre><code>while not done:
    task = pop_local_highest_priority_task()
    if task != NULL:
        execute(task)
    else:
        victim = random_other_thread()
        task = steal_lowest_priority_task_from(victim)
        if task != NULL:
            execute(task)
        else:
            spin_wait()</code></pre>
  </div>

  <div class="section">
    <h2>The Challenge</h2>
    <p>Each thread handles 1000 tasks with mixed priorities. After executing <code>k</code> tasks, it may steal from others. We compare two approaches:</p>
    <ul>
      <li><strong>Thread-local deques:</strong> Efficient but synchronization overhead on status propagation.</li>
    </ul>
    <p>Stealing with priorities can cause inversion if not carefully controlled. We explore ways to balance fairness, correctness, and efficiency under saturation and priority skew.</p>
  </div>

  <div class="section">
    <h2>Resources</h2>
    <p>Our implementation will be in C/C++ and optionally MPI (distributed memory). We refer to literature on lock-free data structures and atomics, and consult documentation on <code>std::atomic</code>, <code>__sync_*</code>, and <code>__atomic_*</code> intrinsics. Testing is done on multi-core machines and GHC clusters.</p>
  </div>

<div class="section">
  <h2>Plan to Achieve</h2>
  <ul>
    <li style="background-color: #d4edda;"><strong>Amogh + Akshay:</strong> Implement a deque with local push/pop and remote steal operations. Initially, the part of stealing the work, updating the queue, and processing elements is done sequentially.</li>
    <li style="background-color: #d4edda;"><strong>Akshay + Amogh:</strong> Introduce lock-free atomic building blocks (e.g., <code>compare_exchange</code>, <code>fetch_add</code>) to avoid synchronization locks across threads in stealing tasks from the deque.</li>
    <li><s><strong>Compare different priority-handling approaches:</strong> separate deques by priority level vs. single-deque combination of priority tagging.</s></li>
    <li><strong>Amogh + Akshay:</strong> Extend the deque to have <code>n</code> deques per processor, each storing tasks tagged with its priority. Operations should be lock-free, and task stealing should choose the best priority task available in that processor.</li>
    <li><strong>Amogh + Akshay:</strong> Extend the stealing logic to choose from the queue populated with the best-priority tasks.</li>
    <li><strong>Akshay:</strong> Measure the tradeoff between stealing 1 task vs. <code>n</code> tasks at once, analyzing time taken, synchronization overhead, communication time, and other relevant metrics.</li>
    <li><strong>Amogh:</strong> Perform extensive benchmarking on performance characteristics such as task throughput, steal attempts, priority inversion, idle time, and scaling behavior.</li>
    <li><strong>Amogh:</strong> Compare tradeoffs when tasks are re-stolen and analyze the resulting overhead and its effect on total execution time.</li>
    <li><strong>Akshay:</strong> Benchmark results against a baseline with random (non-priority) stealing to evaluate the benefit of priority-based approaches.</li>
    <li><em>Nice to have:</em> <strong>Amogh + Akshay:</strong> Implement batching logic where each processor, after executing <code>n</code> tasks, attempts to steal higher-priority tasks from random processors. Evaluate whether this improves high-priority task completion without harming low-priority tasks.</li>
    <li><em>Nice to have:</em> <strong>Akshay + Amogh:</strong> Implement a distributed-memory version using MPI, where each process represents a node with local worker threads and simulates stealing across nodes via message passing.</li>
  </ul>
</div>


  <div class="section">
    <h2>Platform</h2>
    <p>x86-64 multi-core architecture C/C++ for implementation.</p>
  </div>

  <div class="section">
    <h2>Rough Schedule</h2>
  <ul>
    <li><strong><s>Week 1:</s></strong> <s>Setup, approach design finalise</s></li>
    <li><strong><s>Week 2:</s></strong> <s>Lock free implementation for the dequeue complete</s></li>
    <li><strong>Week 3 (First Half):</strong> Extend implementation to handle <code>n</code> deques in a lock-free way with each deque having tasks of different priority.</li>
    <li><strong>Week 3 (Second Half):</strong> Complete the implementation and task stealing logic of stealing the highest priority task from a random processor.</li>
    <li><strong>Week 4 (First Half) (Nice to have):</strong> Implement logic where each processor, after performing <code>n</code> tasks, tries to steal higher-priority tasks from a random processor’s queues.</li>
    <li><strong>Week 4 (Full):</strong> Benchmark all components, generate visualizations, and prepare the final report and poster-day materials. This runs in parallel with the nice-to-have goal, if possible.</li>
  </ul>
</div>


</body>
</html>
